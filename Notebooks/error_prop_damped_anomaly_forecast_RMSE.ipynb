{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>error_prop_damped_anomaly_forecast_RMSE.ipynb</code>.  This notebook calculates the RMSE, MAE, and raw error for the damped anomaly forecast following error propagation after a VRILE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from S2S_sea_ice_metrics import calculate_errors,get_pvalues\n",
    "from S2S_sea_ice_VRILEs import get_VRILE_days_EVENTS\n",
    "from S2S_sea_ice_preprocess import create_obs_climatology,create_aggregate_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The S2S <code>create_aggregate_regions</code> function doesn't work for damped anomaly due to formatting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregate_regions_DAMPED_ANOM(SIE_data):\n",
    "    regions_agg_list = [['Kara Sea','Laptev Sea'],['Barents Sea','Kara Sea','Laptev Sea'],['East Siberian Sea','Beaufort Sea','Chukchi Sea'],\n",
    "                       ['Baffin Bay','East Greenland Sea'],['East Siberian Sea','Beaufort Sea','Chukchi Sea','Laptev Sea']]\n",
    "    region_names_extra = ['Kara-Laptev Sea','Barents-Kara-Laptev Sea','East Siberian-Beaufort-Chukchi Sea',\n",
    "                      'Atlantic','East Siberian-Beaufort-Chukchi-Laptev Sea']\n",
    "    #\n",
    "    for i_reg in np.arange(0,len(regions_agg_list)):\n",
    "        i_reg_sel = regions_agg_list[i_reg]\n",
    "        SIE_ireg = SIE_data[SIE_data['region'].isin(i_reg_sel)]\n",
    "        SIE_ireg_group = SIE_ireg.set_index(['region','init date','valid date','lead time']).sum(level=(1,2,3))\n",
    "        SIE_ireg_group['region'] = region_names_extra[i_reg]\n",
    "        SIE_data =SIE_data.append(SIE_ireg_group.reset_index())\n",
    "        \n",
    "    return(SIE_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Overview of our cross-validated, significance-tested VRILE error. \n",
    "* Load model netCDF files, combine with CTRL, and use common reforecast period. \n",
    "    *  if NCEP, use entire period \n",
    "* Add aggregate regions \n",
    "* Create climatology--model: calculate date of year for valid date, lead time in weeks.\n",
    "    *  Group by region, lead time, and valid date of year \n",
    "    *  Average climatology based on day of year and lead time in weeks--use <code>transform</code> to create <code>SIE_clim</code>.\n",
    "    *  Subtract <code>SIE_clim</code> from <code>SIE</code>\n",
    "* Create observed climatology based on common reforecast period (1999-2014)\n",
    "* Set up LeaveOneOut cross-validation: We remove each year (1999-2014) from the observations.  Then we:\n",
    "    * Calculate VRILEs excluding each year\n",
    "    * Identify forecasts that correspond to VRILE days and separate S2S model data into VRILE days and non-VRILE days\n",
    "    * Calculate errors: as a function of region, valid date, and lead time. \n",
    "    * Assess significance:\n",
    "        * H0: RMSE for non-VRILE days = RMSE for VRILE days \n",
    "        * Calculate p-value: $p_0 = \\frac{(RMSE_{VRILE} - RMSE_{NOVRILE})}{\\sqrt{\\frac{S_{VRILE}^2}{N_{VRILE}} + \\frac{S_{NOVRILE}^2}{N_{NOVRILE}}}}$\n",
    "        * Save p-values, standard deviations as a function of lead time, region, and year left out. \n",
    "        * When $|p| > |p_{crit}|$ ($p_{crit} = \\pm 1.96$), we can say that the model's ability to predict sea ice on VRILE days is significantly different from the model's ability to predict sea ice on non-VRILE days in that region for that lead time while leaving out that year\n",
    "  \n",
    "* How many years must be significantly different for us to say our samples are overall different? Use a binomial test\n",
    "    * $\\sum_{i}^{N} {N \\choose i}p^i(1 - p)^{N - i}$\n",
    "    * N: total number of samples (15, one for each year between 1999-2014)\n",
    "    * p: 0.5 (assume we have equal probability of rejecting or not-rejecting null hypothesis)\n",
    "    * we need to find i: i = 13 for rejecting hypothesis at 95% confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrile_thresh = 0.05\n",
    "thresh_str = '05'\n",
    "nday_change = 5\n",
    "seas_sel = [6,7,8]\n",
    "seas_str = 'JJA'\n",
    "nyear_roll = 10\n",
    "lead_weeks = True\n",
    "COMMON_RF = True\n",
    "VRILE_shift = 21 # days; number of days before a VRILE to analyze\n",
    "max_date_offset = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the damped anomaly model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_name = 'NSIDC_0079'\n",
    "fpath_load = '/home/disk/sipn/mcmcgraw/McGraw_etal_2020/code/make_it_nice/COMMON_LAND_MASK/data/'\n",
    "fname_load = fpath_load+'OBS_{obs_name}/DAMPED_ANOMALY_FORECAST_{nyear_roll}_rolling_mean.csv'.format(obs_name=obs_name,nyear_roll=nyear_roll)\n",
    "SIE_damped_a = pd.read_csv(fname_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SIE_damped_a = create_aggregate_regions_DAMPED_ANOM(SIE_damped_a)\n",
    "SIE_damped_a['init year'] = pd.to_datetime(SIE_damped_a['init date']).dt.year\n",
    "SIE_damped_a['lead time (days)'] = pd.to_timedelta(SIE_damped_a['lead time'],'D')\n",
    "# Trim to common reforecast period\n",
    "#SIE_damped_a = create_aggregate_regions(SIE_damped_a)\n",
    "if COMMON_RF == True:\n",
    "    SIE_damped_a = SIE_damped_a[SIE_damped_a['init year'].isin(np.arange(1999,2015))]\n",
    "SIE_damped = SIE_damped_a.set_index('region')\n",
    "SIE_damped['valid date'] = pd.to_datetime(SIE_damped['valid date'])\n",
    "SIE_damped['model name'] = 'DAMPED ANOMALY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening  <xarray.Dataset>\n",
      "Dimensions:       (nregions: 15, time: 11627)\n",
      "Coordinates:\n",
      "    region_names  (nregions) object dask.array<chunksize=(15,), meta=np.ndarray>\n",
      "  * nregions      (nregions) int64 99 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n",
      "  * time          (time) datetime64[ns] 1989-01-01 1989-01-02 ... 2020-10-31\n",
      "Data variables:\n",
      "    Extent        (time, nregions) float64 dask.array<chunksize=(365, 15), meta=np.ndarray>\n",
      "obs loaded\n"
     ]
    }
   ],
   "source": [
    "if obs_name == 'NSIDC_0079':\n",
    "    obs_type = 'sipn_nc_yearly_agg_commonland'\n",
    "else:\n",
    "    obs_type = 'sipn_nc_yearly_agg'\n",
    "filepath = '/home/disk/sipn/nicway/data/obs/{model_name}/{model_type}/'.format(model_name=obs_name,\n",
    "                                                                              model_type=obs_type)\n",
    "obs_filenames = xr.open_mfdataset(filepath+'/*.nc',combine='by_coords')\n",
    "print('opening ',obs_filenames)\n",
    "obs_SIE = obs_filenames.Extent\n",
    "obs_regions = obs_filenames.nregions\n",
    "obs_region_names = obs_filenames['region_names'].values\n",
    "# Drop region names and re-add as a non-dask.array object.  This is stupid but oh well\n",
    "obs_SIE = obs_SIE.drop('region_names')\n",
    "obs_SIE[\"region_names\"] = (\"nregions\",obs_region_names)\n",
    "print('obs loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add aggregate regions to obs and convert obs to Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_SIE = create_aggregate_regions(obs_SIE)\n",
    "obs_SIE = obs_SIE.to_dataframe().reset_index()\n",
    "obs_SIE = obs_SIE.rename(columns={'Extent':'SIE','region_names':'region','time':'valid date'})\n",
    "obs_SIE['init year'] = pd.to_datetime(obs_SIE['valid date']).dt.year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create climatology.  We use a static mean of the 15 years we include in our data set to match how we treated the S2S data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static climatology\n"
     ]
    }
   ],
   "source": [
    "obs_SIE = obs_SIE[obs_SIE['init year'].isin(np.arange(1999,2015))]\n",
    "#obs_SIE = obs_SIE[obs_SIE['init year'].isin(np.arange(1,2017))]\n",
    "obs_SIE = create_obs_climatology(obs_SIE)\n",
    "print('static climatology')\n",
    "obs_SIE['valid year'] = pd.to_datetime(obs_SIE['valid date']).dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate RMSE based on VRILE events--we want to identify forecasts that start some number of days BEFORE the first day of a VRILE, and watch how the RMSE evolves.  So first, we need to get VRILE days and then identify forecasts that start $n$ days before.  We also need to track VRILE EVENTS--that is, if consecutive days are VRILE days, they are part of the same EVENT.  EVENTS must be separated by <code>BUFFER_DAYS</code> days to be considered separate events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIE_errors_ALL = pd.DataFrame()\n",
    "SIE_anom_errors_ALL = pd.DataFrame()\n",
    "SIE_errors_NO_ALL = pd.DataFrame()\n",
    "SIE_anom_errors_NO_ALL = pd.DataFrame()\n",
    "#\n",
    "#\n",
    "SIE_VRILES_TEST = pd.DataFrame()\n",
    "obs_VRILES_TEST = pd.DataFrame()\n",
    "SIE_no_VRILES_TEST = pd.DataFrame()\n",
    "obs_no_VRILES_TEST = pd.DataFrame()\n",
    "SIE_anom_VRILES_TEST = pd.DataFrame()\n",
    "obs_anom_VRILES_TEST = pd.DataFrame()\n",
    "SIE_anom_no_VRILES_TEST = pd.DataFrame()\n",
    "obs_anom_no_VRILES_TEST = pd.DataFrame()\n",
    "#\n",
    "SIE_reg = SIE_damped\n",
    "SIE_df = SIE_damped.reset_index()\n",
    "#\n",
    "pvalues_SIE = pd.DataFrame()\n",
    "pvalues_SIE_anom = pd.DataFrame()\n",
    "week_length = 7\n",
    "regions_list = SIE_damped_a['region'].unique().tolist()\n",
    "buffer_days = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run LeaveOneOut CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaving out  1999\n",
      "VRILE days calculated\n",
      "errors calculated\n",
      "leaving out  2000\n",
      "VRILE days calculated\n",
      "errors calculated\n",
      "leaving out  2001\n",
      "VRILE days calculated\n",
      "errors calculated\n",
      "leaving out  2002\n",
      "VRILE days calculated\n",
      "errors calculated\n",
      "leaving out  2003\n",
      "VRILE days calculated\n",
      "errors calculated\n",
      "leaving out  2004\n",
      "VRILE days calculated\n",
      "errors calculated\n",
      "leaving out  2005\n",
      "VRILE days calculated\n",
      "errors calculated\n",
      "leaving out  2006\n",
      "VRILE days calculated\n"
     ]
    }
   ],
   "source": [
    "yrs = obs_SIE['valid year'].unique().tolist()\n",
    "for iyr in yrs:\n",
    "    #iyr = 1999\n",
    "    # Remove iyr from obs\n",
    "    obs_SIE_sel = obs_SIE[~obs_SIE['valid year'].isin([iyr])]\n",
    "    print('leaving out ',iyr)\n",
    "    # Estimate observed VRILE days (without iyr)\n",
    "    obs_SIE_VRILE_onlyx, obs_SIE_anom_VRILE_onlyx, obs_SIE_NO_VRILEx, obs_SIE_anom_NO_VRILEx = get_VRILE_days_EVENTS(obs_SIE_sel,vrile_thresh,nday_change,seas_sel,buffer_days)\n",
    "    print('VRILE days calculated')\n",
    "    obs_SIE_VRILE_only = obs_SIE_VRILE_onlyx.set_index(['region'])\n",
    "    obs_SIE_anom_VRILE_only = obs_SIE_anom_VRILE_onlyx.set_index(['region'])\n",
    "    #\n",
    "    obs_SIE_NO_VRILE = obs_SIE_NO_VRILEx.set_index(['region'])\n",
    "    obs_SIE_anom_NO_VRILE = obs_SIE_anom_NO_VRILEx.set_index(['region'])\n",
    "    # Day we START our predictions is determined by VRILE_shift\n",
    "    obs_SIE_VRILE_only['valid date START'] = obs_SIE_VRILE_only['valid date'] - pd.Timedelta(VRILE_shift,'D')\n",
    "    obs_SIE_anom_VRILE_only['valid date START'] = obs_SIE_anom_VRILE_only['valid date'] - pd.Timedelta(VRILE_shift,'D')\n",
    "    # Now, we find model forecasts that start up to max_date_offset days before valid date START\n",
    "    SIE_df['lead time (weeks)'] = np.floor(SIE_df['lead time (days)'].dt.days/week_length)\n",
    "    SIE_df_reg = SIE_df.set_index(['region'])\n",
    "    #x_reg = 'panArctic'\n",
    "    region_list = obs_SIE_sel['region'].unique().tolist()\n",
    "    # Now, we want to compare model forecasts on VRILE days to model forecasts on non-VRILE days\n",
    "    SIE_VRILES = pd.DataFrame()\n",
    "    obs_VRILES = pd.DataFrame()\n",
    "    SIE_no_VRILES = pd.DataFrame()\n",
    "    obs_no_VRILES = pd.DataFrame()\n",
    "    # Same, but for VRILES based on anomalous SIE\n",
    "    SIE_anom_VRILES = pd.DataFrame()\n",
    "    obs_anom_VRILES = pd.DataFrame()\n",
    "    SIE_anom_no_VRILES = pd.DataFrame()\n",
    "    obs_anom_no_VRILES = pd.DataFrame()\n",
    "    # Get SIE forecasts on VRILE and non-VRILE days for each region.  Loop through regions. \n",
    "    for x_reg in regions_list:\n",
    "        # Skip St John because it's crazy\n",
    "        if (x_reg == 'St John') | (x_reg == 'Sea of Okhotsk'):\n",
    "            continue\n",
    "        elif (x_reg == 'Bering') & (seas_str == 'JAS'):\n",
    "            continue\n",
    "        dates_shifted = pd.DataFrame()\n",
    "        dates_shifted['region'] = x_reg\n",
    "        dates_shifted = pd.DataFrame(obs_SIE_VRILE_only.xs(x_reg)['valid date START'])\n",
    "        dates_shifted_list = pd.DataFrame(obs_SIE_VRILE_only.xs(x_reg)['valid date START'])\n",
    "        # Anom dates shifted\n",
    "        anom_dates_shifted = pd.DataFrame()\n",
    "        anom_dates_shifted['region'] = x_reg\n",
    "        anom_dates_shifted = pd.DataFrame(obs_SIE_anom_VRILE_only.xs(x_reg)['valid date START'])\n",
    "        anom_dates_shifted_list = pd.DataFrame(obs_SIE_anom_VRILE_only.xs(x_reg)['valid date START'])\n",
    "        #\n",
    "        for i in np.arange(1,max_date_offset+1):\n",
    "            #i_forward = dates_shifted + pd.Timedelta(i,'D')\n",
    "            i_backward = dates_shifted - pd.Timedelta(i,'D')\n",
    "            dates_shifted_list = dates_shifted_list.append((i_backward))\n",
    "            # same but for anom\n",
    "            #i_anom_forward = anom_dates_shifted + pd.Timedelta(i,'D')\n",
    "            i_anom_backward = anom_dates_shifted - pd.Timedelta(i,'D')\n",
    "            anom_dates_shifted_list = anom_dates_shifted_list.append((i_anom_backward)) \n",
    "        #dates_shifted_list_ALL = dates_shifted_list_ALL.append(dates_shifted_list)\n",
    "        x_SIE = SIE_df_reg.loc[x_reg]\n",
    "        x_SIE_VRILES = x_SIE[pd.to_datetime(x_SIE['init date']).isin(dates_shifted_list['valid date START'])]\n",
    "        x_obs = obs_SIE_sel.set_index('region').loc[x_reg]\n",
    "        x_SIE_obs = x_obs[pd.to_datetime(x_obs['valid date']).isin(x_SIE_VRILES['valid date'])]\n",
    "        SIE_VRILES = SIE_VRILES.append(x_SIE_VRILES)\n",
    "        obs_VRILES = obs_VRILES.append(x_SIE_obs)\n",
    "        #\n",
    "        x_SIE_no = x_SIE[~x_SIE['init date'].isin(dates_shifted_list['valid date START'])]\n",
    "        SIE_no_VRILES = SIE_no_VRILES.append(x_SIE_no)\n",
    "        x_no_obs = x_obs[x_obs['valid date'].isin(x_SIE_no['valid date'])]\n",
    "        obs_no_VRILES = obs_no_VRILES.append(x_no_obs)\n",
    "        ### Same, but for anom\n",
    "        x_anom_SIE_VRILES = x_SIE[pd.to_datetime(x_SIE['init date']).isin(anom_dates_shifted_list['valid date START'])]\n",
    "        #x_anom_obs = obs_SIE.set_index('region').loc[x_reg]\n",
    "        x_anom_SIE_obs = x_obs[pd.to_datetime(x_obs['valid date']).isin(x_anom_SIE_VRILES['valid date'])]\n",
    "        SIE_anom_VRILES = SIE_anom_VRILES.append(x_anom_SIE_VRILES)\n",
    "        obs_anom_VRILES = obs_anom_VRILES.append(x_anom_SIE_obs)\n",
    "        #\n",
    "        x_anom_SIE_no = x_SIE[~x_SIE['init date'].isin(anom_dates_shifted_list['valid date START'])]\n",
    "        SIE_anom_no_VRILES = SIE_anom_no_VRILES.append(x_anom_SIE_no)\n",
    "        x_anom_no_obs = x_obs[x_obs['valid date'].isin(x_anom_SIE_no['valid date'])]\n",
    "        obs_anom_no_VRILES = obs_anom_no_VRILES.append(x_anom_no_obs)\n",
    "        # Calculate RMSE and MAE\n",
    "        if x_reg == 'East Siberian-Beaufort-Chukchi Sea':\n",
    "            SIE_VRILES_TEST = SIE_VRILES_TEST.append(x_SIE_VRILES)\n",
    "            obs_VRILES_TEST = obs_VRILES_TEST.append(x_SIE_obs)\n",
    "            SIE_no_VRILES_TEST = SIE_no_VRILES_TEST.append(x_SIE_no)\n",
    "            obs_no_VRILES_TEST = obs_no_VRILES_TEST.append(x_no_obs)\n",
    "            SIE_anom_VRILES_TEST = SIE_anom_VRILES_TEST.append(x_anom_SIE_VRILES)\n",
    "            obs_anom_VRILES_TEST = obs_anom_VRILES_TEST.append(x_anom_SIE_obs)\n",
    "            SIE_anom_no_VRILES_TEST = SIE_anom_no_VRILES_TEST.append(x_anom_SIE_no)\n",
    "            obs_anom_no_VRILES_TEST = obs_anom_no_VRILES_TEST.append(x_anom_no_obs)\n",
    "    if lead_weeks == True:\n",
    "        clim_freq_str = 'WEEKLY'\n",
    "        SIE_VRILES['lead days'] = SIE_VRILES['lead time (weeks)']\n",
    "        SIE_anom_VRILES['lead days'] = SIE_anom_VRILES['lead time (weeks)']\n",
    "        SIE_raw_err,SIE_errors = calculate_errors(SIE_VRILES.reset_index(),obs_VRILES.reset_index())\n",
    "        SIE_anom_raw_err,SIE_anom_errors = calculate_errors(SIE_anom_VRILES.reset_index(),obs_anom_VRILES.reset_index())\n",
    "        ## NO VRILES\n",
    "        SIE_no_VRILES['lead days'] = SIE_no_VRILES['lead time (weeks)']\n",
    "        SIE_anom_no_VRILES['lead days'] = SIE_anom_no_VRILES['lead time (weeks)']\n",
    "        SIE_raw_err_NO,SIE_errors_NO = calculate_errors(SIE_no_VRILES.reset_index(),obs_SIE_sel)\n",
    "        SIE_anom_raw_err_NO,SIE_anom_errors_NO = calculate_errors(SIE_anom_no_VRILES.reset_index(),\n",
    "                                                                  obs_anom_no_VRILES.reset_index())\n",
    "    else:\n",
    "        clim_freq_str = 'DAILY'\n",
    "        SIE_VRILES['lead days'] = SIE_VRILES['lead time (days)'].dt.days\n",
    "        SIE_anom_VRILES['lead days'] = SIE_anom_VRILES['lead time (days)'].dt.days\n",
    "        SIE_raw_err,SIE_errors = calculate_errors(SIE_VRILES,obs_shifted_dates)\n",
    "        SIE_anom_raw_err,SIE_anom_errors = calculate_errors(SIE_anom_VRILES,obs_anom_shifted_dates)\n",
    "        ## NO VRILES\n",
    "        SIE_no_VRILES['lead days'] = SIE_no_VRILES['lead time (days)'].dt.days\n",
    "        SIE_anom_no_VRILES['lead days'] = SIE_anom_no_VRILES['lead time (days)'].dt.days\n",
    "        SIE_raw_err_NO,SIE_errors_NO = calculate_errors(SIE_no_VRILES,obs_no_shifted_dates)\n",
    "        SIE_anom_raw_err_NO,SIE_anom_errors_NO = calculate_errors(SIE_anom_no_VRILES,obs_anom_no_shifted_dates)\n",
    "    print('errors calculated')\n",
    "    #\n",
    "    # Get p-values\n",
    "    sd_VRILE,sd_noVRILE,p_value,N_vrile,N_novrile = get_pvalues(SIE_VRILES,SIE_no_VRILES,SIE_errors,SIE_errors_NO)\n",
    "    sd_VRILE_anom,sd_noVRILE_anom,p_value_anom,N_vrile_anom,N_novrile_anom = get_pvalues(SIE_anom_VRILES,\n",
    "                                                            SIE_anom_no_VRILES,SIE_anom_errors,SIE_anom_errors_NO)\n",
    "    # \n",
    "    # Add information to dataframes\n",
    "    SIE_errors['year out'] = iyr\n",
    "    SIE_errors['SIE sdev'] = sd_VRILE\n",
    "    SIE_errors['sample size'] = N_vrile\n",
    "    SIE_errors['p-value'] = p_value\n",
    "    SIE_errors_NO['year out'] = iyr\n",
    "    SIE_errors_NO['SIE sdev'] = sd_noVRILE\n",
    "    SIE_errors_NO['sample size'] = N_novrile\n",
    "    SIE_errors_NO['p-value'] = p_value\n",
    "    #\n",
    "    SIE_anom_errors['year out'] = iyr\n",
    "    SIE_anom_errors['SIE sdev'] = sd_VRILE_anom\n",
    "    SIE_anom_errors['sample size'] = N_vrile_anom\n",
    "    SIE_anom_errors['p-value'] = p_value_anom\n",
    "    SIE_anom_errors_NO['year out'] = iyr\n",
    "    SIE_anom_errors_NO['SIE sdev'] = sd_noVRILE_anom\n",
    "    SIE_anom_errors_NO['sample size'] = N_novrile_anom\n",
    "    SIE_anom_errors_NO['p-value'] = p_value_anom\n",
    "    # Append each CV slice to full data set\n",
    "    SIE_errors_ALL = SIE_errors_ALL.append(SIE_errors)\n",
    "    SIE_anom_errors_ALL = SIE_anom_errors_ALL.append(SIE_anom_errors)\n",
    "    SIE_errors_NO_ALL = SIE_errors_NO_ALL.append(SIE_errors_NO)\n",
    "    SIE_anom_errors_NO_ALL = SIE_anom_errors_NO_ALL.append(SIE_anom_errors_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIE_anom_errors_ALL.xs('Bering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SIE_errors_ALL.xs('Kara-Laptev Sea').reset_index().plot.scatter(x='lead days',y='SIE anom RMSE',color='r')\n",
    "SIE_errors_NO.xs('Kara-Laptev Sea').reset_index().plot.scatter(x='lead days',y='SIE RMSE',color='b')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot RMSE of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "ax1 = fig1.add_axes([0,0,1,1])\n",
    "model_name = 'DAMPED_ANOMALY_{obs_name}'.format(obs_name=obs_name)\n",
    "reg_sel = 'East Siberian-Beaufort-Chukchi Sea'\n",
    "foo = SIE_errors_ALL.xs(reg_sel).reset_index()\n",
    "foo.plot.scatter(x='lead days',y=['SIE anom RMSE'],linewidth=2,linestyle='--',ax=ax1,color='r',label=['VRILES'])\n",
    "foo2 = SIE_errors_NO_ALL.xs(reg_sel).reset_index()\n",
    "foo2.plot.scatter(x='lead days',y=['SIE anom RMSE'],linewidth=2,linestyle=':',ax=ax1,label=['NO VRILES'])\n",
    "#\n",
    "figpath_save = '/home/disk/sipn/mcmcgraw/McGraw_etal_2020/code/make_it_nice/COMMON_LAND_MASK/figures/diagnostics/'\n",
    "fig1.suptitle('RMSE on VRILE and nonVRILE Days, anomalous SIE, {region}, {model_name}, {seas_str}'.format(region=reg_sel,\n",
    "                                                                               model_name=model_name,seas_str=seas_str),fontsize=15,\n",
    "             y=1.1)\n",
    "\n",
    "fpath_save_fig1 = figpath_save+'ERROR_PROP_{VRILE_shift}day_shift_damped_anom_SIE_anom_RMSE_all_slices_{reg_sel}_{model_name}_{seas_str}.png'.format(VRILE_shift=VRILE_shift,reg_sel=reg_sel,\n",
    "                                                                            model_name=model_name,seas_str=seas_str)\n",
    "fig1.savefig(fpath_save_fig1,format='png',dpi=350,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(2)\n",
    "ax2 = fig2.add_axes([0,0,1,1])\n",
    "foo = SIE_errors_ALL.xs(reg_sel).reset_index()\n",
    "foo.plot.scatter(x='lead days',y=['SIE RMSE'],linewidth=2,linestyle='--',ax=ax2,color='r',label=['VRILES'])\n",
    "foo2 = SIE_errors_NO_ALL.xs(reg_sel).reset_index()\n",
    "foo2.plot.scatter(x='lead days',y=['SIE RMSE'],linewidth=2,linestyle=':',ax=ax2,label=['NO VRILES'])\n",
    "#\n",
    "fig2.suptitle('RMSE on VRILE and nonVRILE Days, SIE, {region}, {model_name}, {seas_str}'.format(region=reg_sel,\n",
    "                                                                               model_name=model_name,\n",
    "                                                        seas_str=seas_str),fontsize=15,y=1.1)\n",
    "\n",
    "fpath_save_fig2 = figpath_save+'ERROR_PROP_{VRILE_shift}day_shift_damped_anom_SIE_RMSE_all_slices_{reg_sel}_{model_name}_{seas_str}.png'.format(VRILE_shift=VRILE_shift,reg_sel=reg_sel,\n",
    "                                                                            model_name=model_name,seas_str=seas_str)\n",
    "fig2.savefig(fpath_save_fig2,format='png',dpi=350,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot p-value of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig3 = plt.figure(3)\n",
    "ax3 = fig3.add_axes([0,0,1,1])\n",
    "foo = SIE_errors_ALL.xs(reg_sel).reset_index()\n",
    "sns.scatterplot(data=foo,x='lead days',y='p-value',hue='year out')\n",
    "ax3.axhline(-1.96,color='k')\n",
    "ax3.axhline(1.96,color='k')\n",
    "SIE_errors_ALL_masked = SIE_errors_ALL.mask(SIE_errors_ALL['p-value'].abs()<1.96)\n",
    "foo2 = SIE_errors_ALL_masked.xs(reg_sel).reset_index()\n",
    "sns.scatterplot(data=foo2,x='lead days',y='p-value',hue='year out',s=200,legend=False,alpha=0.25)\n",
    "fig3.suptitle('p-values, VRILE days vs non-VRILE days, {reg_sel}, {model_name}, {seas_str}'.format(reg_sel=reg_sel,\n",
    "                                                            model_name=model_name,seas_str=seas_str),fontsize=15,y=1.1)\n",
    "fpath_save_fig3 = figpath_save+'ERROR_PROP_{VRILE_shift}day_shift_pvalues_each_fold_{reg_sel}_{model_name}_{seas_str}.png'.format(VRILE_shift=VRILE_shift,reg_sel=reg_sel,\n",
    "                                                                    model_name=model_name,seas_str=seas_str)\n",
    "fig3.savefig(fpath_save_fig3,format='png',dpi=350,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine everything together into one data frame. Replace <code>SIE anom RMSE</code> and <code>SIE anom MAE</code> in <code>SIE_errors</code> with corresponding entries from  <code>SIE_anom_errors</code> (and same for raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIE_anom_errors_ALL = SIE_anom_errors_ALL.rename(columns={'p-value':'p-value anom'})\n",
    "SIE_anom_errors_NO_ALL = SIE_anom_errors_NO_ALL.rename(columns={'p-value':'p-value anom'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIE_errors_VRILE = SIE_errors_ALL.copy()\n",
    "SIE_errors_VRILE = SIE_errors_VRILE.drop(columns={'SIE anom RMSE','SIE anom MAE'})\n",
    "SIE_errors_VRILE = SIE_errors_VRILE.join(SIE_anom_errors_ALL[['SIE anom RMSE','SIE anom MAE','p-value anom']])\n",
    "SIE_errors_VRILE['type'] = 'VRILE days'\n",
    "#\n",
    "SIE_errors_noVRILE = SIE_errors_NO_ALL.copy()\n",
    "SIE_errors_noVRILE = SIE_errors_noVRILE.drop(columns={'SIE anom RMSE','SIE anom MAE'})\n",
    "SIE_errors_noVRILE = SIE_errors_noVRILE.join(SIE_anom_errors_NO_ALL[['SIE anom RMSE','SIE anom MAE','p-value anom']])\n",
    "SIE_errors_noVRILE['type'] = 'no VRILE days'\n",
    "#\n",
    "SIE_errors_FULL = SIE_errors_VRILE.append(SIE_errors_noVRILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIE_errors_FULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = '/home/disk/sipn/mcmcgraw/McGraw_etal_2020/code/make_it_nice/COMMON_LAND_MASK/data/{model_name}/'.format(model_name=model_name)\n",
    "fdir = fdir+'OBS_{obs_name}/'.format(obs_name=obs_name)\n",
    "if COMMON_RF == True:\n",
    "    fdir = fdir+'COMMON_RF/'\n",
    "else:\n",
    "    fdir = fdir+'FULL_TIME/'\n",
    "if nday_change != 5:\n",
    "    fdir = fdir+'VRILEs_{nday_change}day_change/'.format(nday_change=nday_change)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "#\n",
    "\n",
    "fname_save_RMSE = fdir+'ERROR_PROP_{VRILE_shift}day_shift_VRILE_vs_NOVRILE_RMSE_MAE_{model_name}_months{seas_str}_VRILE{thresh_str}_model_clim_freq_{clim_freq_str}.csv'.format(VRILE_shift=VRILE_shift,model_name=model_name,\n",
    "                                             seas_str=seas_str,thresh_str=thresh_str,clim_freq_str=clim_freq_str)\n",
    "#fname_save_raw = fdir+'RAW_err_{model_name}_months{seas_str}_VRILE{thresh_str}_model_clim_freq_{clim_freq_str}.csv'.format(model_name=model_name,\n",
    "#                                             seas_str=seas_str,thresh_str=thresh_str,clim_freq_str=clim_freq_str)\n",
    "#\n",
    "#SIE_raw_err_FULL.to_csv(fname_save_raw)\n",
    "SIE_errors_FULL.to_csv(fname_save_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_save_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIE_anom_errors_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sea_ice_variability_S2S",
   "language": "python",
   "name": "sea_ice_variability_s2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
